\documentclass[letterpaper]{amsart}
\usepackage{amsmath, amssymb}
\usepackage{tikz}
\usetikzlibrary{graphs}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}

\begin{document}
\title[Homework 1]{Homework Assignment 1 \\ OR750: Deep Learning, Fall 2018}
\author{David Prentiss}
%\email{dprentiss@gmail.com}
%\urladdr{https://github.com/dprentiss/OR750}
\date{October 15, 2018}
\maketitle

% problem 1
\section*{Exercise 1}
Let $H$ be the hypothesis that the disease is present and $D$ be the data
representing a positive test result. The sensitivity and specificity of the test
are given as
$\Pr(D|H) = 0.99$
and
$\Pr(\neg D|\neg H) = 0.99$
respectively.
The prevelance of the disease is 1 in 10,000 or
$\Pr(H) = 0.0001$.
We wish to find
$\Pr(H|D)$.

From Bayes' theorem
\begin{equation*}
  \Pr(H\mid D) = \frac{\Pr(D\mid H)\Pr(H)}{\Pr(D)}.
\end{equation*}
From the law of total probability
\begin{equation*}
  \Pr(D) = \sum_{h_i\in\Omega_h}\Pr(D\mid h_i)\Pr(h_i)
  = \Pr(D\mid H)\Pr(H) +\Pr(D\mid\neg H)\Pr(\neg H).
\end{equation*}
Then we need to find
\begin{equation*}
  \Pr(\neg H) = 1 - \Pr(H) = 0.9999
\end{equation*}
and
\begin{equation*}
  \Pr(D\mid\neg H) = 1 - \Pr(D\mid H) = 0.01.
\end{equation*}
Then
\begin{align*}
  \Pr(H\mid D)
  &= \frac{\Pr(D\mid H)\Pr(H)}{\Pr(D)}
  \\
  &= \frac{\Pr(D\mid H)\Pr(H)}{\Pr(D\mid H)\Pr(H) +\Pr(D\mid\neg H)\Pr(\neg H)}
  \\
  &= \frac{0.99(0.0001)}{0.99(0.0001) + 0.01(0.9999)} \approx 0.98\%
\end{align*}

% problem 2
\section*{Exercise 2}
From the given conditional probability tables we construct the following causal
relationship graph.

\begin{figure}
  \tikz \graph [layered layout] {
    "$v$" -> "$h$";
    "$d$" -> "$c$" -> "$t$";
    "$v$" -> "$c$";
  };
  \caption{Graph of causal relationships for $\Pr(t|c)$, $\Pr(h|v)$, and $\Pr(c|d,v)$.}
\end{figure}

Let
$\Pr(x = 1)$
be denoted
$\Pr(X)$
for all random variables $x$ in this problem. We need to find
$\Pr(V| H)$,
$\Pr(V| T)$
and
$\Pr(V| H, T)$.
From Bayes' theorem we know that
\begin{align}
  \Pr(V\mid H) &= \frac{\Pr(H\mid V)\Pr(V)}{\Pr(H)} \\
  \Pr(V\mid T) &= \frac{\Pr(T\mid V)\Pr(V)}{\Pr(T)} \\
  \Pr(V\mid H, T) &= \frac{\Pr(H, T\mid V)\Pr(V)}{\Pr(H,T)} \label{eq:vht}
\end{align}
Also, noting that $h$ and $t$ are conditionally independent, equation \ref{eq:vht} becomes
\begin{equation}
  \Pr(V\mid H, T) = \frac{\Pr(H\mid V)\Pr(V)\Pr(T\mid V)\Pr(V)}{\Pr(H)\Pr(T)}.
\end{equation}
The probabilities
$\Pr(V)$,
$\Pr(D)$,
$\Pr(T|c)$,
$\Pr(H|v)$,
and
$\Pr(C|d,v)$
are given. To find total probabilities $\Pr(H)$ and $\Pr(T)$
we apply the law of total probability as before.
\begin{align*}
  \Pr(H) &= \Pr(H\mid V)\Pr(V) +\Pr(H\mid\neg V)\Pr(\neg V)\\
  &=
\end{align*}
and
\begin{align*}
  \Pr(T) &= \Pr(T\mid C)\Pr(C) +\Pr(T\mid\neg C)\Pr(\neg C).
\end{align*}
So, we must also find the total probability $\Pr(C)$.
Again, applying the law of total probability we have

\begin{align*}
  \Pr(C) &= \sum_{d_i\Omega_d}\sum_{v_i\in\Omega_v}\Pr(C\mid d_i, v_i)\Pr(d_i,v_i) \\
  \begin{split}
    &= \Pr(C\mid D, V)\Pr(D, V)
    + \Pr(C\mid D, \neg V)\Pr(D, \neg V)
    \\
    &\qquad + \Pr(C\mid \neg D, V)\Pr(\neg D, V)
    + \Pr(C\mid \neg D, \neg V)\Pr(\neg D, \neg V)
  \end{split}
  \\
         &=
\end{align*}
Finally, we must find $\Pr(T|V)$. Consider the joint probability $\Pr(C,D,T,V)$.
We choose this probability because it contains all conditional variables up our givens.
From the chain rule of probabilities we know that
\begin{equation*}
  \Pr(C,D,T,V) &= \Pr(T\mid C,D,V)\Pr(C\mid D,V)\Pr(D\mid V)\Pr(V). \\
\end{equation*}
And since $d$ and $v$ are conditionally independent, we have
\begin{equation*}
  \Pr(C,D,T,V) &= \Pr(T\mid C,D,V)\Pr(C\mid D,V)\Pr(D)\Pr(V). \\
\end{equation*}

\begin{equation*}
  \Pr(C,D,T,V) &= \Pr(C\mid D,T,V)\Pr(D\mid T,V)\Pr(T\mid V)\Pr(V). \\
\end{equation*}
\begin{equation*}
  \Pr(C,D,T,V) &= \Pr(C\mid D,V)\Pr(D)\Pr(T\mid V)\Pr(V). \\
\end{equation*}
% problem 6
\section{}

\newpage
\begin{align}
  \Pr(\mu\mid\tau, y)
  &\propto
    \Pr(y\mid\tau, \mu)
    \Pr(\mu)
  \\
  &\propto
    \prod_{i=1}^n
    \left[
    \exp\left(-\frac{\left(y_i-\mu\right)^2}{2\tau^{-1}}\right)
    \right]
    \times
    \exp\left(-\frac{1}{2}\mu^2\right)
  \\
  \ln
  \Pr(\mu\mid\tau, y)
  &\propto
    \sum_{i=1}^n
    \left[
    -\frac{\left(y_i-\mu\right)^2}{2\tau^{-1}}
    \right]
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{\tau}{2}
    \sum_{i=1}^n
    \left(y_i-\mu\right)^2
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{\tau}{2}
    \sum_{i=1}^n
    \left(\mu^2-2\mu y_i + y_i^2\right)
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{\tau}{2}
    \sum_{i=1}^n \mu^2
    +\tau\mu\sum_{i=1}^n y_i
    -\frac{\tau}{2}
    \sum_{i=1}^n y_i^2
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{1}{2}
    \tau n \mu^2
    +\tau n\bar{y}\mu
    -\frac{\tau}{2}
    \sum_{i=1}^n y_i^2
    -\frac{1}{2}\mu^2
  \\
  \label{break1}
  &\propto
    -\frac{1}{2}
    \left(1+\tau n\right) \mu^2
    +\tau n\bar{y}\mu
    -\frac{\tau}{2}
    \sum_{i=1}^n y_i^2
\end{align}
Now, if we let
\begin{align}
  a &=
      -\frac{1}{2}
      \left(1+\tau n\right)
  \\
  b &=
      \tau n\bar{y}
  \\
  c &=
      -\frac{\tau}{2}
      \sum_{i=1}^n y_i^2
\end{align}
the right-hand side of equation \ref{break1} may be written as a polynomial of the form
$a\mu^2 + b\mu + c$.
After completing the square, we have
$a\left(\mu - h\right)^2+k$,
where
\begin{align}
  h &= -\frac{b}{2a}
      = \frac{\tau n\bar{y}}{1+\tau n}
\end{align}
and
\begin{align}
  k &= c-\frac{b^2}{4a}
      = -\frac{\tau}{2}
      \sum_{i=1}^n y_i^2
      +\frac{\tau^2 n^2\bar{y}^2}{2\left(1+\tau n\right)}
\end{align}
Note that $k$, lacking any terms containing $\mu$, is a constant.
We can then rewrite equation \ref{break1} as
\begin{align}
  \ln
  \Pr(\mu\mid\tau, y)
  %&\propto
    %-\frac{1}{2}
    %\left(1+\tau n\right) \mu^2
    %+\tau n\bar{y}\mu
    %-\frac{\tau}{2}
    %\sum_{i=1}^n y_i^2
  %\\
  &\propto
    -\frac{1}{2}
    \left(1+\tau n\right)
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    + k
    %+\frac{\tau^2 n^2\bar{y}^2}{2\left(1+\tau n\right)}
    %-\frac{\tau}{2}
    %\sum_{i=1}^n y_i^2
  \\
  \Pr(\mu\mid\tau, y)
  &\propto
    \exp\left[
    -\frac{1}{2}
    \left(1+\tau n\right)
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    \right]
    \times
    e^k
    %\exp\left[
    %\frac{\tau^2 n^2\bar{y}^2}{2\left(1+\tau n\right)}
    %-\frac{\tau}{2}
    %\sum_{i=1}^n y_i^2
    %\right]
  \\
  &\propto
    \exp\left[
    -\frac{1}{2}
    \left(1+\tau n\right)
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    \right]
  \\
  &\propto
    \exp\left[
    -\frac{
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    }{
    2\left(\frac{1}{1+\tau n}\right)
    }
    \right].
\end{align}
So then
\begin{equation}
  \mu\mid\tau,y
  \sim \mathcal{N}\left(\frac{\tau n\bar{y}}{1+\tau n},
\frac{1}{1+\tau n}\right)
\end{equation}
\section*{Exercise 8}
\begin{equation*}
  f(\theta)
  =
  \frac{1}{m}
  \sum_{i=1}^m
  \left[
    -y^{(i)}
    \ln\left(h_\theta(x^{(i)})\right)
    -(1 - y^{(i)})
    \ln\left(1-h_\theta(x^{(i)})\right)
  \right]
  + \frac{\lambda}{2m}
  \|\theta}\|_2^2
\end{equation*}
\begin{align*}
  \frac{\partial}{\partial\theta_k}
  f(\theta)
  &=
  \frac{-y^{(i)}}{m}
  \sum_{i=1}^m
  \left[
  \frac{\partial}{\partial\theta_k}
    \ln\left(h_\theta(x^{(i)})\right)
  \right]
  +
  \frac{y^{(i)}-1}{m}
  \sum_{i=1}^m
  \left[
  \frac{\partial}{\partial\theta_k}
    \ln\left(1 - h_\theta(x^{(i)})\right)
  \right]
  + \frac{\lambda\theta_i}{m}
  \\
  &=
  \frac{-y^{(i)}}{m}
  \sum_{i=1}^m
  \left[
    \frac{1}{h_\theta(x^{(i)})}
  \frac{\partial}{\partial\theta_k}
    h_\theta(x^{(i)})
  \right]
  +
  \frac{y^{(i)}-1}{m}
  \sum_{i=1}^m
  \left[
    \frac{-1}{1 - h_\theta\left(x^{(i)}\right)}
  \frac{\partial}{\partial\theta_k}
    h_\theta(x^{(i)})
  \right]
  + \frac{\lambda\theta_i}{m}
\end{align*}
Consider the product
\begin{equation*}
\theta\tran x^{(i)}
= \sum_{j=1}^3\theta_jx_j^{(i)}
= \theta_kx_k^{(i)}
+ \sum_{j\neq k}\theta_jx_j^{(i)}
\end{equation*}
and let
\begin{equation*}
\sum_{j\neq k}\theta_jx_j^{(i)} = C.
\end{equation*}
Then,
\begin{align*}
  h_\theta(x^{(i)})
  &=g(\theta\tran x^{(i)})
  = \frac{1}{1 + \exp\left(-\theta_kx_k^{(i)}\right)
    \exp\left(-C\right)}
    \\
  \frac{\partial}{\partial\theta_k}
  h_\theta(x^{(i)})
  &= \frac{\partial}{\partial\theta_k}
  \left[
    1 + \exp\left(-\theta_kx_k^{(i)}\right)
    \exp\left(-C\right)
  \right]^{-1}
  \\
  &=
  -\left[
    1 + \exp\left(-\theta_kx_k^{(i)}\right)
    \exp \left(-C\right)
  \right]^{-2}
  \left[
    -x_k^{(i)}
    \exp\left(-\theta_kx_k^{(i)}\right)
    \exp\left(-C\right)
  \right].
\end{align*}
\end{document}

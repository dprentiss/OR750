\documentclass[letterpaper]{amsart}
\usepackage{amsmath, amssymb}
\usepackage{tikz}
\usetikzlibrary{graphs}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}

\begin{document}
\title[Homework 1]{Homework Assignment 1 \\ OR750: Deep Learning, Fall 2018}
\author{David Prentiss}
%\email{dprentiss@gmail.com}
%\urladdr{https://github.com/dprentiss/OR750}
\date{October 15, 2018}
\maketitle

% problem 1
\section*{Exercise 1}
Let $H$ be the hypothesis that the disease is present and $D$ be the data
representing a positive test result. The sensitivity and specificity of the test
are given as
$\Pr(D|H) = 0.99$
and
$\Pr(\neg D|\neg H) = 0.99$
respectively.
The prevelance of the disease is 1 in 10,000 or
$\Pr(H) = 0.0001$.
We wish to find
$\Pr(H|D)$.

From Bayes' theorem
\begin{equation*}
  \Pr(H\mid D) = \frac{\Pr(D\mid H)\Pr(H)}{\Pr(D)}.
\end{equation*}
From the law of total probability
\begin{equation*}
  \Pr(D) = \sum_{h_i\in\Omega_h}\Pr(D\mid h_i)\Pr(h_i)
  = \Pr(D\mid H)\Pr(H) +\Pr(D\mid\neg H)\Pr(\neg H).
\end{equation*}
Then we need to find
\begin{equation*}
  \Pr(\neg H) = 1 - \Pr(H) = 0.9999
\end{equation*}
and
\begin{equation*}
  \Pr(D\mid\neg H) = 1 - \Pr(D\mid H) = 0.01.
\end{equation*}
Then
\begin{align*}
  \Pr(H\mid D)
  &= \frac{\Pr(D\mid H)\Pr(H)}{\Pr(D)}
  \\
  &= \frac{\Pr(D\mid H)\Pr(H)}{\Pr(D\mid H)\Pr(H) +\Pr(D\mid\neg H)\Pr(\neg H)}
  \\
  &= \frac{0.99(0.0001)}{0.99(0.0001) + 0.01(0.9999)} \approx 0.98\%
\end{align*}

% problem 2
\section*{Exercise 2}
From the given conditional probability tables we can construct the following causal
relationship graph.
\begin{figure}[h]
  \tikz \graph [layered layout] {
    "$v$" -> "$h$";
    "$d$" -> "$c$" -> "$t$";
    "$v$" -> "$c$";
  };
  \caption{Graph of causal relationships for $\Pr(t|c)$, $\Pr(h|v)$, and $\Pr(c|d,v)$.}
\end{figure}

Let
$\Pr(x = 1)$
be denoted
$\Pr(X)$
for all random variables $x$ in this problem. We need to find
$\Pr(V| H)$,
$\Pr(V| T)$
and
$\Pr(V| H, T)$.
From Bayes' theorem we know that
\begin{align}
  \Pr(V\mid H) &= \frac{\Pr(H\mid V)\Pr(V)}{\Pr(H)} \\
  \Pr(V\mid T) &= \frac{\Pr(T\mid V)\Pr(V)}{\Pr(T)} \\
  \Pr(V\mid H, T) &= \frac{\Pr(H, T\mid V)\Pr(V)}{\Pr(H,T)} \label{eq:vht}
\end{align}
Also, noting that $h$ and $t$ are conditionally independent, equation \ref{eq:vht} becomes
\begin{equation}
  \Pr(V\mid H, T) = \frac{\Pr(H\mid V)\Pr(V)\Pr(T\mid V)\Pr(V)}{\Pr(H)\Pr(T)}.
\end{equation}
The probabilities
$\Pr(V)$,
$\Pr(D)$,
$\Pr(T|c)$,
$\Pr(H|v)$,
and
$\Pr(C|d,v)$
are given. To find total probabilities $\Pr(H)$ and $\Pr(T)$
we apply the law of total probability as before.
\begin{align*}
  \Pr(H) &= \Pr(H\mid V)\Pr(V) +\Pr(H\mid\neg V)\Pr(\neg V)\\
  &=
\end{align*}
and
\begin{align*}
  \Pr(T) &= \Pr(T\mid C)\Pr(C) +\Pr(T\mid\neg C)\Pr(\neg C).
\end{align*}
So, we must also find the total probability $\Pr(C)$.
Again, applying the law of total probability we have

\begin{align*}
  \Pr(C) &= \sum_{d_i\Omega_d}\sum_{v_i\in\Omega_v}\Pr(C\mid d_i, v_i)\Pr(d_i,v_i) \\
  \begin{split}
    &= \Pr(C\mid D, V)\Pr(D, V)
    + \Pr(C\mid D, \neg V)\Pr(D, \neg V)
    \\
    &\qquad + \Pr(C\mid \neg D, V)\Pr(\neg D, V)
    + \Pr(C\mid \neg D, \neg V)\Pr(\neg D, \neg V)
  \end{split}
  \\
         &=
\end{align*}
Finally, we must find $\Pr(T|V)$. Consider the joint probability $\Pr(C,D,T,V)$.
We choose this probability because it contains all conditional variables up our givens.
From the chain rule of probabilities we know that
\begin{equation*}
  \Pr(C,D,T,V) = \Pr(T\mid C,D,V)\Pr(C\mid D,V)\Pr(D\mid V)\Pr(V). \\
\end{equation*}
And since $d$ and $v$ are conditionally independent, we have
\begin{equation*}
  \Pr(C,D,T,V) = \Pr(T\mid C,D,V)\Pr(C\mid D,V)\Pr(D)\Pr(V). \\
\end{equation*}

\begin{equation*}
  \Pr(C,D,T,V) = \Pr(C\mid D,T,V)\Pr(D\mid T,V)\Pr(T\mid V)\Pr(V). \\
\end{equation*}
\begin{equation*}
  \Pr(C,D,T,V) = \Pr(C\mid D,V)\Pr(D)\Pr(T\mid V)\Pr(V). \\
\end{equation*}

\section*{Exercise 3}
Let
\(x_i\sim \text{Pois}(\lambda)}, i = 1,\dots,N\).
Estimate \(\lambda\) using maximum likelihood estimation.
\begin{align*}
  l(\lambda)
  &= \sum_{i=1}^N\ln f(x_i \mid \lambda)
  = \sum_{i=1}^N\ln \frac{\lambda^x e^{-\lambda}}{x_i!}
  = \sum_{i=1}^N \left[x_i\ln\lambda-\lambda-\ln(x_i!) \right]
    \\
  &= \ln\lambda\sum_{i=1}^N x_i -N\lambda-\sum_{i=1}^N\ln(x_i!)
\end{align*}
Since
$l(\lambda)$
is convex upward, it is maximized where
$l^\prime(\lambda) = 0$.
So,  the MLE estimate,
\(\lambda_{\text{ML}}\),
is
\begin{align*}
    l^\prime(\lambda_{\text{ML}})
  =
  \frac{1}
  {\lambda_{\text{ML}}}
    \sum_{i=1}^N x_i
    -N
  &= 0
  \\
  \lambda_{\text{ML}}
  &=
    \frac{1}{N}
    \sum_{i=1}^N x_i
    = \bar{x}
\end{align*}
\section*{Exercise 4}
Let
\(x_i\sim \text{Pois}(\lambda)}, i = 1,\dots,N\),
as before.
Find
\(\Pr(\lambda|1,\dots,N)\),
assuming a prior distribution
\(\lambda\sim\Gamma(\lambda|a,b)\).
\begin{align*}
  \Pr(\lambda\mid x_1,\dots, x_N)
  &\propto
    \Pr(x_1,\dots, x_N\mid\lambda)\Pr(\lambda)
    \\
  &\propto
    \prod_{i=1}^Nf(x_i\mid\lambda)f(\lambda\mid\alpha,\beta)
    \\
  \ln\Pr(\lambda\mid x_1,\dots, x_N)
  &\propto
   \sum_{i=1}^N\ln \frac{\lambda^x e^{-\lambda}}{x_i!}
    + (\alpha - 1)\ln\lambda-\beta\lambda
    \\
  &\propto
    \ln\lambda\sum_{i=1}^N x_i -N\lambda
    -\sum_{i=1}^N\ln(x_i!)
    + (\alpha - 1)\ln\lambda-\beta\lambda
    \\
  &\propto
    \left(
    \sum_{i=1}^N x_i+\alpha-1
    \right)
    \ln\lambda
    -(N + \beta)\lambda
    -\sum_{i=1}^N\ln(x_i!)
    \\
  \Pr(\lambda\mid x_1,\dots, x_N)
  &\propto
    \lambda^{
    \left(
    N\bar{x}+\alpha-1
    \right)
    }
    e^{
    -(N + \beta)\lambda
    }
    e^{
    \left(
    -\sum_{i=1}^N\ln(x_i!)
      \right)
    }
    \\
  &\propto
    \lambda^{
    \left(
    N\bar{x}+\alpha-1
    \right)
    }
    e^{
    -(N + \beta)\lambda
    }
  \\
  &=
    \frac{
    (N+\beta)^{(N\bar{x}+\alpha)}
    \lambda^{(N\bar{x}+\alpha-1)}
    e^{-(N + \beta)\lambda}
    }
    {
    \Gamma\left(
    N\bar{x}+\alpha
    \right)
    }
\end{align*}

\section*{Exercise 5}
Let
\(x_1,x_2,\dots,x_N\)
be an independent sample from the exponential distribution with density
\(
Pr(x|\lambda) = \lambda e^{-\lambda x}
\).
Find the maximum likelihood estimate,
\(\lambda_\text{ML}\).
\begin{align*}
  l(\lambda)
  &= \sum_{i=1}^N\ln f(x_i \mid \lambda)
    = N\ln\lambda - \lambda N\bar{x}
\end{align*}
Since
$l(\lambda)$
is convex upward, it is maximized where
$l^\prime(\lambda) = 0$.
So the MLE estimate,
\(\lambda_{\text{ML}}\),
is
\begin{align*}
  l^\prime
  (\lambda_{\text{ML}})
  = \frac{N}{\lambda_{\text{ML}}} - N\bar{x}
  &=0
    \\
  \lambda_{\text{ML}}
  &= \frac
  {N}
  {N\bar{x}}
  = \frac
  {1}
  {\bar{x}}
\end{align*}
Choosing a gamma distribution for our conjugate prior we have
\begin{align*}
  \Pr(\lambda\mid x_1,\dots, x_N)
  &\propto
    \Pr(x_1,\dots, x_N\mid\lambda)\Pr(\lambda)
    \\
  &\propto
    \prod_{i=1}^Nf(x_i\mid\lambda)f(\lambda\mid\alpha,\beta)
    \\
  &\propto
    \prod_{i=1}^N\left(\lambda e^{-\lambda x_i}\right)
    \left( \lambda^{\alpha-1} e^{-\beta\lambda} \right)
    \\
  \ln\Pr(\lambda\mid x_1,\dots, x_N)
  &\propto
   \sum_{i=1}^N \left( \ln \lambda e^{-\lambda x_i} \right)
    + (\alpha - 1)\ln\lambda-\beta\lambda
    \\
  &\propto
    N\ln\lambda - \lambda \sum_{i=1}^N x_i
    + (\alpha - 1)\ln\lambda-\beta\lambda
    \\
  &\propto
    \left(
    N+\alpha-1
    \right)
    \ln\lambda
    -(N\bar{x} + \beta)\lambda
    \\
  \Pr(\lambda\mid x_1,\dots, x_N)
  &\propto
    \lambda^{
    \left(
    N+\alpha-1
    \right)
    }
    e^{
    -(N\bar{x}+ \beta)\lambda
    }
    \\
  &=
    \frac{
    (N\bar{x}+\beta)^{(N+\alpha)}
    \lambda^{(N+\alpha-1)}
    e^{-(N\bar{x} + \beta)\lambda}
    }
    {
    \Gamma\left(
    N\bar{x}+\beta
    \right)
    }
\end{align*}
So the posterior distribution of \(\lambda\) is a gamma distribution with
\(\lambda\sim\Gamma(N\bar{x}+\beta, N+\alpha)\).
since the expected value, \(\text{E}\left[X\right]\),
of a gamma-distributed variable \(X\sim\Gamma(\alpha,\beta)\) is
\begin{equation*}
  \text{E}\left[X\right]
  =
  \frac{\alpha}{\beta}
\end{equation*}
then the expected value of \(\lambda\) is
\begin{equation*}
  \text{E}\left[\lambda\right]
  =
  \frac
  {N\bar{x}+\beta}
  {N+\alpha}
\end{equation*}


% problem 6
\section*{Exercise 6}
\begin{align}
  \Pr(\mu\mid\tau, y)
  &\propto
    \Pr(y\mid\tau, \mu)
    \Pr(\mu)
  \\
  &\propto
    \prod_{i=1}^n
    \left[
    \exp\left(-\frac{\left(y_i-\mu\right)^2}{2\tau^{-1}}\right)
    \right]
    \times
    \exp\left(-\frac{1}{2}\mu^2\right)
  \\
  \ln
  \Pr(\mu\mid\tau, y)
  &\propto
    \sum_{i=1}^n
    \left[
    -\frac{\left(y_i-\mu\right)^2}{2\tau^{-1}}
    \right]
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{\tau}{2}
    \sum_{i=1}^n
    \left(y_i-\mu\right)^2
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{\tau}{2}
    \sum_{i=1}^n
    \left(\mu^2-2\mu y_i + y_i^2\right)
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{\tau}{2}
    \sum_{i=1}^n \mu^2
    +\tau\mu\sum_{i=1}^n y_i
    -\frac{\tau}{2}
    \sum_{i=1}^n y_i^2
    -\frac{1}{2}\mu^2
  \\
  &\propto
    -\frac{1}{2}
    \tau n \mu^2
    +\tau n\bar{y}\mu
    -\frac{\tau}{2}
    \sum_{i=1}^n y_i^2
    -\frac{1}{2}\mu^2
  \\
  \label{break1}
  &\propto
    -\frac{1}{2}
    \left(1+\tau n\right) \mu^2
    +\tau n\bar{y}\mu
    -\frac{\tau}{2}
    \sum_{i=1}^n y_i^2
\end{align}
Now, if we let
\begin{align}
  a &=
      -\frac{1}{2}
      \left(1+\tau n\right)
  \\
  b &=
      \tau n\bar{y}
  \\
  c &=
      -\frac{\tau}{2}
      \sum_{i=1}^n y_i^2
\end{align}
the right-hand side of equation \ref{break1} may be written as a polynomial of the form
$a\mu^2 + b\mu + c$.
After completing the square, we have
$a\left(\mu - h\right)^2+k$,
where
\begin{align}
  h &= -\frac{b}{2a}
      = \frac{\tau n\bar{y}}{1+\tau n}
\end{align}
and
\begin{align}
  k &= c-\frac{b^2}{4a}
      = -\frac{\tau}{2}
      \sum_{i=1}^n y_i^2
      +\frac{\tau^2 n^2\bar{y}^2}{2\left(1+\tau n\right)}
\end{align}
Note that $k$, lacking any terms containing $\mu$, is a constant.
We can then rewrite equation \ref{break1} as
\begin{align}
  \ln
  \Pr(\mu\mid\tau, y)
  %&\propto
    %-\frac{1}{2}
    %\left(1+\tau n\right) \mu^2
    %+\tau n\bar{y}\mu
    %-\frac{\tau}{2}
    %\sum_{i=1}^n y_i^2
  %\\
  &\propto
    -\frac{1}{2}
    \left(1+\tau n\right)
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    + k
    %+\frac{\tau^2 n^2\bar{y}^2}{2\left(1+\tau n\right)}
    %-\frac{\tau}{2}
    %\sum_{i=1}^n y_i^2
  \\
  \Pr(\mu\mid\tau, y)
  &\propto
    \exp\left[
    -\frac{1}{2}
    \left(1+\tau n\right)
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    \right]
    \times
    e^k
    %\exp\left[
    %\frac{\tau^2 n^2\bar{y}^2}{2\left(1+\tau n\right)}
    %-\frac{\tau}{2}
    %\sum_{i=1}^n y_i^2
    %\right]
  \\
  &\propto
    \exp\left[
    -\frac{1}{2}
    \left(1+\tau n\right)
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    \right]
  \\
  &\propto
    \exp\left[
    -\frac{
    \left(\mu - \frac{\tau n\bar{y}}{1+\tau n}\right)^2
    }{
    2\left(\frac{1}{1+\tau n}\right)
    }
    \right].
\end{align}
So then
\begin{equation}
  \mu\mid\tau,y
  \sim \mathcal{N}\left(\frac{\tau n\bar{y}}{1+\tau n},
\frac{1}{1+\tau n}\right)
\end{equation}
\section*{Exercise 7}
\section*{Exercise 8}
\begin{equation*}
  f(\theta)
  =
  \frac{1}{m}
  \sum_{i=1}^m
  \left[
    -y^{(i)}
    \ln\left(h_\theta(x^{(i)})\right)
    -(1 - y^{(i)})
    \ln\left(1-h_\theta(x^{(i)})\right)
  \right]
  + \frac{\lambda}{2m}
  \|\theta\|_2^2
\end{equation*}
\begin{align*}
  \frac{\partial}{\partial\theta_k}
  f(\theta)
  &=
  \frac{1}{m}
  \sum_{i=1}^m
    \left[
    -y^{(i)}
  \frac{\partial}{\partial\theta_k}
    \ln\left(h_\theta(x^{(i)})\right)
  -
    (1 - y^{(i)})
  \frac{\partial}{\partial\theta_k}
    \ln\left(1 - h_\theta(x^{(i)})\right)
  \right]
  + \frac{\lambda\theta_k}{m}
  \\
  &=
  \frac{1}{m}
  \sum_{i=1}^m
    \left[
    \frac
    {-y^{(i)}}
    {h_\theta(x^{(i)})}
  \frac{\partial}{\partial\theta_k}
    h_\theta(x^{(i)})
  +
    \frac
    {1 - y^{(i)}}
    {1 - h_\theta\left(x^{(i)}\right)}
  \frac{\partial}{\partial\theta_k}
    h_\theta(x^{(i)})
  \right]
  + \frac{\lambda\theta_k}{m}
\end{align*}
Consider the product
\begin{equation*}
\theta\tran x^{(i)}
= \sum_{j=1}^3\theta_jx_j^{(i)}
= \theta_kx_k^{(i)}
+ \sum_{j\neq k}\theta_jx_j^{(i)}
\end{equation*}
and let
\begin{equation*}
C:=\sum_{j\neq k}\theta_jx_j^{(i)}
\end{equation*}
Then,
\begin{align*}
  h_\theta(x^{(i)})
  =g(\theta\tran x^{(i)})
  &= \frac{1}{1 + \exp\left(-\theta \tran x^{(i)}\right)}
    \\
  &= \frac{1}
    {1 + \exp\left(-\theta_kx_k^{(i)}\right)
    \exp\left(-C\right)}
    \\
  \frac{\partial}{\partial\theta_k}
  h_\theta(x^{(i)})
  &=
  \frac{\partial}{\partial\theta_k}
  \left[
    1 + \exp\left(-\theta_kx_k^{(i)}\right)
    \exp\left(-C\right)
  \right]^{-1}
  \\
  &=
  -\left[
    1 + \exp\left(-\theta_kx_k^{(i)}\right)
    \exp \left(-C\right)
  \right]^{-2}
  \left[
    -x_k^{(i)}
    \exp\left(-\theta_kx_k^{(i)}\right)
    \exp\left(-C\right)
  \right]
    \\
  &=
  -\left[
    1 + \exp\left(-\theta\tran x^{(i)}\right)
  \right]^{-2}
  \left[
    -x_k^{(i)}
    \exp\left(-\theta\tran x^{(i)}\right)
  \right]
    \\
  &=
    x_k^{(i)}
    \exp\left(-\theta\tran x^{(i)}\right)
  \left[
    1 + \exp\left(-\theta\tran x^{(i)}\right)
  \right]^{-2}
\end{align*}
\end{document}
